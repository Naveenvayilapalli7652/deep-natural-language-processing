{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Memory Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9sTjqVL/zFw+ZPR2cKqZY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcecode369/deep-nlp/blob/master/memory%20networks/Memory_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAQALE_CCZ7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, print_function, unicode_literals, division\n",
        "from builtins import range, input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbpKb3PpYSFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline \n",
        "import re \n",
        "import tarfile\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Input, Lambda, Reshape, add, dot, Activation \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvQtSXQSZNYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = get_file('babi-tasks-v1-2.tar.gz',\n",
        "                origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
        "\n",
        "tar = tarfile.open(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD5ROsULZs7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "challenges = {\n",
        "    'single_supporting_fact_10k':'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
        "    'two_supporting_fact_10k':'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGGfk7MTaT1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(sent):\n",
        "  return [x.strip() for x in re.split('(\\W+)?',sent) if x.strip()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bta0uyNtamJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_stories(f):\n",
        "  data = []\n",
        "  story = []\n",
        "  printed = False\n",
        "  for line in f:\n",
        "    line = line.decode('utf-8').strip()\n",
        "    nid, line = line.split(' ', 1)\n",
        "    if int(nid) == 1:\n",
        "      story = []\n",
        "    if '\\t' in line:\n",
        "      q, a, supporting = line.split('\\t')\n",
        "      q = tokenize(q)\n",
        "      story_so_far = [[str(i)] + s for i, s in enumerate(story) if s]\n",
        "      data.append((story_so_far, q, a))\n",
        "      story.append('')\n",
        "    else:\n",
        "      story.append(tokenize(line))\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbkFgUm8bvmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def should_flatten(el):\n",
        "  return not isinstance(el, (str, bytes))\n",
        "\n",
        "def flatten(l):\n",
        "  for el in l:\n",
        "    if should_flatten(el):\n",
        "      yield from flatten(el)\n",
        "    else:\n",
        "      yield el"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G49NVvvWcMH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_stories(data, word2idx, story_maxlen, query_maxlen):\n",
        "  inputs, queries, answers = [], [], []\n",
        "  for story, query, answer in data:\n",
        "    inputs.append([[word2idx[w] for w in s] for s in story])\n",
        "    queries.append([word2idx[w] for w in query])\n",
        "    answers.append([word2idx[answer]])\n",
        "  return (\n",
        "    [pad_sequences(x, maxlen=story_maxlen) for x in inputs],\n",
        "    pad_sequences(queries, maxlen=query_maxlen),\n",
        "    np.array(answers)\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F2JRWqXerXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stack_inputs(inputs, story_maxsents, story_maxlen):\n",
        "  for i, story in enumerate(inputs):\n",
        "    inputs[i] = np.concatenate(\n",
        "        [\n",
        "         story, \n",
        "         np.zeros((story_maxsents-story.shape[0], story_maxlen),'int')\n",
        "        ]\n",
        "    )\n",
        "  return np.stack(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA0HHRoSfxHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(challenge_type):\n",
        "  challenge = challenges[challenge_type]\n",
        "  \n",
        "  train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
        "  test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
        "  \n",
        "  stories = train_stories + test_stories\n",
        "  \n",
        "  story_maxlen = max((len(s) for x, _, _ in stories for s in x))\n",
        "  story_maxsents = max((len(x) for x, _, _ in stories))\n",
        "  query_maxlen = max(len(x) for _, x, _ in stories)\n",
        "\n",
        "  vocab = sorted(set(flatten(stories)))\n",
        "  vocab.insert(0, '<PAD>')\n",
        "  vocab_size = len(vocab)\n",
        "\n",
        "  word2idx = {c:i for i, c in enumerate(vocab)}\n",
        "\n",
        "  inputs_train, queries_train, answers_train = vectorize_stories(\n",
        "      train_stories,\n",
        "      word2idx,\n",
        "      story_maxlen,\n",
        "      query_maxlen\n",
        "  )\n",
        "  inputs_test, queries_test, answers_test = vectorize_stories(\n",
        "      test_stories, \n",
        "      word2idx,\n",
        "      story_maxlen,\n",
        "      query_maxlen\n",
        "  )\n",
        "  inputs_train = stack_inputs(inputs_train, story_maxsents, story_maxlen)\n",
        "  inputs_test = stack_inputs(inputs_test, story_maxsents, story_maxlen)\n",
        "  print(f\"inputs_train.shape {inputs_train.shape}, inputs_test.shape {inputs_test.shape}\")\n",
        "  return train_stories, test_stories, inputs_train, queries_train, answers_train, \\\n",
        "  inputs_test, queries_test, answers_test, story_maxsents, story_maxlen, query_maxlen, vocab, vocab_size "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQvw8syFiFzF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ed98115-efd2-48eb-892c-e2f76f3e6db1"
      },
      "source": [
        "train_stories, test_stories, inputs_train, queries_train, answers_train, \\\n",
        "  inputs_test, queries_test, answers_test, story_maxsents, story_maxlen, query_maxlen, vocab, vocab_size = get_data('single_supporting_fact_10k')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs_train.shape (10000, 10, 8), inputs_test.shape (1000, 10, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyR1p8i3iUtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}