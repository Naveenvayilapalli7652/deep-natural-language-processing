{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation with Bahdanau-Attention and Pretrained Glove Embeddings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwA3SlIMpsklHErVIVRPLF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcecode369/deep-nlp/blob/master/machine%20translation/Neural_Machine_Translation_with_Bahdanau_Attention_and_Pretrained_Glove_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIjQ1GZNtX2g",
        "colab_type": "code",
        "outputId": "446a127b-e686-4807-ee27-b3ddb1799d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, print_function, division, unicode_literals\n",
        "from builtins import range, input\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import urllib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.ticker as ticker\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set(rc={\"figure.figsize\":(12,10)})\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import glob\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "import unicodedata\n",
        "\n",
        "import gc\n",
        "gc.enable()\n",
        "\n",
        "import warnings \n",
        "warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTWUuQjX3bu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Can use either of two dataset. To benchmark and check performance of Neural Machine Translation from seq to seq and Neural Machine Translation using Attention Mechanism we're going to use the same dataset.\n",
        "# Download the dataset from  https://www.manythings.org/anki/\n",
        "# [OPTIONAL] if youre using google colab.\n",
        "\n",
        "# try:\n",
        "#   print('Uploading file...')\n",
        "#   from google.colab import files\n",
        "#   files.upload()\n",
        "#   print(\"File uploaded sucessfully.\")\n",
        "# except Exception as ex:\n",
        "#   print(f\"Process terminated with exception: {ex}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq975qFs7gV9",
        "colab_type": "code",
        "outputId": "50edad08-e46a-4450-b834-57956582ba5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "## This is another dataset for Neural Machine Translation hosted by google. Either of the dataset can work. \n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('spa-eng.zip',origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',extract=True)\n",
        "path_to_file = os.path.dirname(path_to_zip) + \"/spa-eng/spa.txt\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLfzN__18DP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6IQh_mK8TuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "  w = re.sub(r\"([?.!,])\",r\" \\1 \",w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,]+\",\" \", w)\n",
        "\n",
        "  w = w.rstrip().strip()\n",
        "  w = \"<start> \" + w + \" <end>\"\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jErs4FjJ8jIq",
        "colab_type": "code",
        "outputId": "e761d730-ee91-4a2d-ba65-b13cf6ad9ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "<start> puedo tomar prestado este libro ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NikFMxRV9qJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split(\"\\n\")\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVOHkLRC59DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4d0025cd-47d9-4397-d6eb-aa265209c604"
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(f\"{en[-1]}\")\n",
        "print(f\"{sp[-1]}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqJTZTkL6ELR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "08247da4-973f-4e7b-9947-da1579caeebf"
      },
      "source": [
        "print(len(en))\n",
        "print(len(sp))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118964\n",
            "118964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xK7PRoR6TF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)\n",
        "\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "  return tensor, lang_tokenizer \n",
        "\n",
        "def load_dataset(path, num_examples=None):\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, target_lang_tokenizer = tokenize(targ_lang)\n",
        "  cache = {\"input_tensor\":input_tensor, \"target_tensor\":target_tensor, \"inp_lang_tokenizer\":inp_lang_tokenizer, \"target_lang_tokenizer\":target_lang_tokenizer}\n",
        "  return cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "594p8NF3fD-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = 40000\n",
        "cache = load_dataset(path_to_file, num_examples)\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = cache[\"input_tensor\"], cache[\"target_tensor\"], cache[\"inp_lang_tokenizer\"], cache[\"target_lang_tokenizer\"]\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppq1m3ENfq8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c68d167b-7812-408a-921c-e2185f4126e0"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val),len(target_tensor_val))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32000 32000 8000 8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX_AG1t-gN6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print(\"%d -----> %s\"%(t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E4XERMOgeOp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "d2cac7ab-b6ae-42f2-f9d5-04b7b00b6eca"
      },
      "source": [
        "print(f\"Input language; index to word mapping: {convert(inp_lang,input_tensor_train[0])}\")\n",
        "print()\n",
        "print(f\"Target language; index to word mapping: {convert(targ_lang,target_tensor_train[0])}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 -----> <start>\n",
            "22 -----> te\n",
            "130 -----> gustan\n",
            "30 -----> los\n",
            "9843 -----> cappelletis\n",
            "4 -----> ?\n",
            "2 -----> <end>\n",
            "Input language; index to word mapping: None\n",
            "\n",
            "1 -----> <start>\n",
            "19 -----> do\n",
            "5 -----> you\n",
            "34 -----> like\n",
            "5225 -----> tortellini\n",
            "6 -----> ?\n",
            "2 -----> <end>\n",
            "Target language; index to word mapping: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clKM6kvshmKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "embedding_dim = 100\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index) + 1\n",
        "vocab_tar_size = len(targ_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).cache().shuffle(BUFFER_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1gw1QzMijEu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ad7796c-17d3-46e3-bc68-2224ea1c28b3"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 12]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASuo07Wvhlso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "e90d63b9-56c3-4ee9-ee8e-10c767d5a02c"
      },
      "source": [
        "import urllib\n",
        "print('Downloading pretrained embedding vectors..')\n",
        "urllib.request.urlretrieve(\"https://github.com/allenai/spv2/blob/master/model/glove.6B.100d.txt.gz?raw=true\",filename=\"glove.6B.100d.txt.gz\")\n",
        "\n",
        "print('Extracting pretrained embedding vectors...')\n",
        "import gzip\n",
        "import shutil\n",
        "with gzip.open('glove.6B.100d.txt.gz', 'rb') as f_in:\n",
        "    with open('glove.6B.100d.txt', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "print('Loading pretrained embedding...')\n",
        "word2vec = {}\n",
        "with open('glove.6B.100d.txt') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:],dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print(f\"Found {len(word2vec)} word vectors\")\n",
        "\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "print('Filling pretrained embeddings...')\n",
        "num_words = min(MAX_NUM_WORDS, len(inp_lang.word_index)+1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in inp_lang.word_index.items():\n",
        "  if i < MAX_NUM_WORDS:\n",
        "    embedding_vector = inp_lang.word_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector      \n",
        "print('Done.!')\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix], input_length=max_length_inp)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading pretrained embedding vectors..\n",
            "Extracting pretrained embedding vectors...\n",
            "Loading pretrained embedding...\n",
            "Found 400000 word vectors\n",
            "Filling pretrained embeddings...\n",
            "Done.!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48_8PnIci-0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = embedding \n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A92TbFcgkNU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_size=vocab_inp_size, embedding=embedding_layer, enc_units=units, batch_sz=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZFvNKJUknbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-4tTIf9k77g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dbc52412-ceef-4962-f675-4c7b5af6dd58"
      },
      "source": [
        "print(f\"Encoder output shape: (batch_size, sequence_length, units) {sample_output.shape}\")\n",
        "print(f\"Encoder hidden state shape: (batch_size, units) {sample_hidden.shape}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch_size, sequence_length, units) (64, 16, 1024)\n",
            "Encoder hidden state shape: (batch_size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux5yxJheHak0",
        "colab_type": "text"
      },
      "source": [
        "### Bahdanau Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAjAw9Q-Gxrp",
        "colab_type": "text"
      },
      "source": [
        "![bahdanau attention](https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg)\n",
        "\n",
        "The input is put through an encoder model which gives us the encoder output of shape (batch_size, max_length, hidden_size) and the encoder hidden state of shape (batch_size, hidden_size).\n",
        "\n",
        "Here are the equations that are implemented:\n",
        "\n",
        "![bahdanau attention equations](https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg)\n",
        "![bahdanau attention equation final](https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg)\n",
        "\n",
        "This tutorial uses Bahdanau attention for the encoder. Let's decide on notation before writing the simplified form:\n",
        "\n",
        "    FC = Fully connected (dense) layer\n",
        "    EO = Encoder output\n",
        "    H = hidden state\n",
        "    X = input to the decoder\n",
        "\n",
        "\n",
        "And the pseudo-code:\n",
        "\n",
        "1. `score = FC(tanh(FC(EO) + FC(H)))`\n",
        "\n",
        "2. `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the 1st axis, since the shape of score is (batch_size, max_length, hidden_size). Max_length is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
        "\n",
        "3. `context vector = sum(attention weights * EO, axis = 1).` Same reason as above for choosing axis as 1.\n",
        "\n",
        "4. `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
        "\n",
        "5. `merged vector = concat(embedding output, context vector)`\n",
        "\n",
        "6. This merged vector is then given to the GRU\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gymT-To4lQ53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(self.W1(values)+self.W2(hidden_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1TTY8iWJo7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a2bec923-a0b4-45c1-c4f1-cff5403f51f3"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "print(\"Attention result shape: (batch_size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch_size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVU-_4LgKKKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    \n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "    \n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector,1), x],axis=-1)\n",
        "    \n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "    \n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    \n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JPYea6wKaLa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fa5c9c4-712e-4adc-b2f4-eb342a3772f1"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_deocoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "\n",
        "print('Decoder output shape: (batch_size, vocab_size) {}'.format(sample_deocoder_output.shape))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab_size) (64, 5934)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h62uVUfODbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype = loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-DzeagfOtTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgeGik_VO-vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3_e9ehtZ9Dv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fc9f58e-5ac8-4d75-d145-827b29d8da11"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.5928\n",
            "Epoch 1 Batch 100 Loss 2.1154\n",
            "Epoch 1 Batch 200 Loss 1.9155\n",
            "Epoch 1 Batch 300 Loss 1.9647\n",
            "Epoch 1 Batch 400 Loss 2.1029\n",
            "Epoch 1 Loss 2.1080\n",
            "Time taken for 1 epoch 52.636006355285645 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.9071\n",
            "Epoch 2 Batch 100 Loss 1.8438\n",
            "Epoch 2 Batch 200 Loss 1.7932\n",
            "Epoch 2 Batch 300 Loss 1.6950\n",
            "Epoch 2 Batch 400 Loss 1.7676\n",
            "Epoch 2 Loss 1.7253\n",
            "Time taken for 1 epoch 42.948100328445435 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.5064\n",
            "Epoch 3 Batch 100 Loss 1.5018\n",
            "Epoch 3 Batch 200 Loss 1.5530\n",
            "Epoch 3 Batch 300 Loss 1.5983\n",
            "Epoch 3 Batch 400 Loss 1.5363\n",
            "Epoch 3 Loss 1.5484\n",
            "Time taken for 1 epoch 42.71331524848938 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.4557\n",
            "Epoch 4 Batch 100 Loss 1.3371\n",
            "Epoch 4 Batch 200 Loss 1.4291\n",
            "Epoch 4 Batch 300 Loss 1.4986\n",
            "Epoch 4 Batch 400 Loss 1.3848\n",
            "Epoch 4 Loss 1.4432\n",
            "Time taken for 1 epoch 42.61167526245117 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4508\n",
            "Epoch 5 Batch 100 Loss 1.3826\n",
            "Epoch 5 Batch 200 Loss 1.3970\n",
            "Epoch 5 Batch 300 Loss 1.4027\n",
            "Epoch 5 Batch 400 Loss 1.4719\n",
            "Epoch 5 Loss 1.3640\n",
            "Time taken for 1 epoch 42.58864235877991 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3380\n",
            "Epoch 6 Batch 100 Loss 1.3971\n",
            "Epoch 6 Batch 200 Loss 1.2076\n",
            "Epoch 6 Batch 300 Loss 1.2786\n",
            "Epoch 6 Batch 400 Loss 1.2192\n",
            "Epoch 6 Loss 1.3021\n",
            "Time taken for 1 epoch 42.66327357292175 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.2397\n",
            "Epoch 7 Batch 100 Loss 1.1660\n",
            "Epoch 7 Batch 200 Loss 1.2312\n",
            "Epoch 7 Batch 300 Loss 1.2560\n",
            "Epoch 7 Batch 400 Loss 1.2625\n",
            "Epoch 7 Loss 1.2500\n",
            "Time taken for 1 epoch 42.506192684173584 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.1886\n",
            "Epoch 8 Batch 100 Loss 1.2049\n",
            "Epoch 8 Batch 200 Loss 1.2708\n",
            "Epoch 8 Batch 300 Loss 1.1742\n",
            "Epoch 8 Batch 400 Loss 1.2509\n",
            "Epoch 8 Loss 1.2028\n",
            "Time taken for 1 epoch 42.557350635528564 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.0698\n",
            "Epoch 9 Batch 100 Loss 1.0657\n",
            "Epoch 9 Batch 200 Loss 1.2541\n",
            "Epoch 9 Batch 300 Loss 1.1273\n",
            "Epoch 9 Batch 400 Loss 1.1579\n",
            "Epoch 9 Loss 1.1653\n",
            "Time taken for 1 epoch 42.538129568099976 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.1022\n",
            "Epoch 10 Batch 100 Loss 1.2782\n",
            "Epoch 10 Batch 200 Loss 1.1701\n",
            "Epoch 10 Batch 300 Loss 1.1428\n",
            "Epoch 10 Batch 400 Loss 1.2294\n",
            "Epoch 10 Loss 1.1293\n",
            "Time taken for 1 epoch 42.61816382408142 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBFqleVP5Us2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}